{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M0hammadTamimi/1-1-2025-SQuAD-dataset/blob/main/Untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vExbeKG7e4Mp"
      },
      "source": [
        "!pip install transformers datasets openpyxl\n",
        "!pip install datasets\n",
        "!pip install -q transformers datasets wandb peft torch pandas openpyxl\n",
        "# Cell 1: Install Requirements\n",
        "!pip install -q transformers datasets wandb peft torch pandas openpyxl psutil tkseem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II7gq1Noe4Mu"
      },
      "source": [
        "# Cell 1: Install Requirements (same)\n",
        "!pip install -q transformers datasets wandb peft torch pandas openpyxl psutil tkseem\n",
        "\n",
        "# Cell 2: Import Libraries (same, plus CUDA optimizations)\n",
        "import pandas as pd\n",
        "import wandb\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from datasets import Dataset, load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from peft import get_peft_model, LoraConfig\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import subprocess\n",
        "import psutil\n",
        "\n",
        "# Enable CUDA optimizations\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# Cell 3: Mount Drive and Setup (same)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "wandb.login()\n",
        "wandb.init(project=\"huggingface\", entity=\"mohammadtamimi300-hashmite-tech\")\n",
        "\n",
        "# Cell 4: Display System Info (same)\n",
        "def display_system_info():\n",
        "    gpu_info = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n",
        "    print(gpu_info.stdout.decode())\n",
        "    ram_info = psutil.virtual_memory()\n",
        "    print(f\"Total RAM: {ram_info.total / (1024 ** 3):.2f} GB\")\n",
        "    print(f\"Available RAM: {ram_info.available / (1024 ** 3):.2f} GB\")\n",
        "\n",
        "display_system_info()\n",
        "\n",
        "# Cell 5: Load Datasets (same)\n",
        "print(\"Loading SQuAD dataset...\")\n",
        "squad_dataset = load_dataset(\"squad\")\n",
        "\n",
        "print(\"Loading custom dataset...\")\n",
        "df = pd.read_excel('/content/datasetQA.xlsx')\n",
        "print(f\"Custom dataset loaded with {len(df)} rows\")\n",
        "\n",
        "# Cell 6: Initialize Tokenizer (optimized)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"aubmindlab/bert-base-arabertv2\",\n",
        "    use_fast=True,\n",
        "    model_max_length=512\n",
        ")\n",
        "\n",
        "# Cell 7: Preprocessing Functions (same logic, optimized implementation)\n",
        "def preprocess_squad(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    contexts = [c.strip() for c in examples[\"context\"]]\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        contexts,\n",
        "        max_length=512,\n",
        "        truncation=\"only_second\",\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None  # Changed for batch processing\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        sample_idx = sample_map[i]\n",
        "        answer = examples[\"answers\"][sample_idx]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = start_char + len(answer[\"text\"][0])\n",
        "\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        context_start = sequence_ids.index(1) if 1 in sequence_ids else -1\n",
        "        context_end = sequence_ids.index(1, context_start + 1) if 1 in sequence_ids[context_start + 1:] else len(sequence_ids) - 1\n",
        "\n",
        "        if context_start == -1 or offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs\n",
        "\n",
        "def preprocess_custom_dataset(examples):\n",
        "    questions = [str(q) if pd.notnull(q) else '' for q in examples[\"question\"]]\n",
        "    contexts = [str(c) if pd.notnull(c) else '' for c in examples[\"context\"]]\n",
        "    answers = examples['answer']\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        contexts,\n",
        "        max_length=512,\n",
        "        truncation=\"only_second\",\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None  # Changed for batch processing\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        sample_idx = sample_map[i]\n",
        "        answer = answers[sample_idx]\n",
        "        if not answer:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "            continue\n",
        "\n",
        "        context = contexts[sample_idx]\n",
        "        start_char = context.find(answer)\n",
        "        end_char = start_char + len(answer)\n",
        "\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        context_start = sequence_ids.index(1) if 1 in sequence_ids else -1\n",
        "        context_end = sequence_ids.index(1, context_start + 1) if 1 in sequence_ids[context_start + 1:] else len(sequence_ids) - 1\n",
        "\n",
        "        if start_char == -1 or context_start == -1 or offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs\n",
        "\n",
        "# Cell 8: Process Datasets (optimized)\n",
        "print(\"Processing SQuAD dataset...\")\n",
        "processed_squad = squad_dataset.map(\n",
        "    preprocess_squad,\n",
        "    remove_columns=squad_dataset[\"train\"].column_names,\n",
        "    batched=True,\n",
        "    batch_size=1000,  # Increased batch size\n",
        "    num_proc=4  # Use multiple CPU cores\n",
        ")\n",
        "\n",
        "print(\"Processing custom dataset...\")\n",
        "custom_dataset = Dataset.from_pandas(df)\n",
        "processed_custom = custom_dataset.map(\n",
        "    preprocess_custom_dataset,\n",
        "    remove_columns=custom_dataset.column_names,\n",
        "    batched=True,\n",
        "    batch_size=1000,  # Increased batch size\n",
        "    num_proc=4  # Use multiple CPU cores\n",
        ")\n",
        "\n",
        "# Split custom dataset (same)\n",
        "custom_train, custom_val = processed_custom.train_test_split(test_size=0.1).values()\n",
        "\n",
        "# Cell 9: Initialize Model with LoRA (fixed)\n",
        "print(\"Initializing model...\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\n",
        "    \"aubmindlab/bert-base-arabertv2\",\n",
        "    return_dict=True,\n",
        "    # Remove torch_dtype=torch.float16 from here\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"QUESTION_ANS\",\n",
        "    target_modules=[\"query\", \"key\", \"value\"],\n",
        "    bias=\"none\",\n",
        "    modules_to_save=[\"qa_outputs\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Cell 10: Training Configuration (fixed)\n",
        "def get_training_args(output_dir, name):\n",
        "    return TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        run_name=name,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=100,\n",
        "        logging_steps=50,\n",
        "        learning_rate=5e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        gradient_accumulation_steps=2,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        report_to=\"wandb\",\n",
        "        fp16=True,  # Keep this\n",
        "        # Remove fp16_opt_level=\"O2\"\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=200,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        warmup_ratio=0.1,\n",
        "        group_by_length=True,\n",
        "        dataloader_num_workers=2,  # Reduced to avoid warning\n",
        "        gradient_checkpointing=False,\n",
        "        optim=\"adamw_torch\"\n",
        "    )\n",
        "\n",
        "# Cell 11: Training (fixed)\n",
        "print(\"Training on SQuAD...\")\n",
        "squad_args = get_training_args('/content/squad_model', \"squad_pretraining\")\n",
        "squad_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=squad_args,\n",
        "    train_dataset=processed_squad[\"train\"],\n",
        "    eval_dataset=processed_squad[\"validation\"],\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        ")\n",
        "\n",
        "squad_trainer.train()\n",
        "\n",
        "# Clear cache between training phases\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nFine-tuning on custom dataset...\")\n",
        "custom_args = get_training_args('/content/final_model', \"custom_finetuning\")\n",
        "custom_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=custom_args,\n",
        "    train_dataset=custom_train,\n",
        "    eval_dataset=custom_val,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        ")\n",
        "\n",
        "custom_trainer.train()\n",
        "\n",
        "# Clear cache between training phases\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nFine-tuning on custom dataset...\")\n",
        "custom_args = get_training_args('/content/final_model', \"custom_finetuning\")\n",
        "custom_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=custom_args,\n",
        "    train_dataset=custom_train,\n",
        "    eval_dataset=custom_val,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        ")\n",
        "\n",
        "custom_trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install Requirements\n",
        "!pip install -q transformers datasets wandb peft torch pandas openpyxl psutil tkseem\n",
        "\n",
        "# Cell 2: Import Libraries\n",
        "import pandas as pd\n",
        "import wandb\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from datasets import Dataset, load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from peft import get_peft_model, LoraConfig\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import subprocess\n",
        "import psutil\n",
        "\n",
        "# Enable CUDA optimizations\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# Cell 3: Mount Drive and Setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "wandb.login()\n",
        "wandb.init(project=\"huggingface\", entity=\"mohammadtamimi300-hashmite-tech\")\n",
        "\n",
        "# Cell 4: Display System Info\n",
        "def display_system_info():\n",
        "    gpu_info = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n",
        "    print(gpu_info.stdout.decode())\n",
        "    ram_info = psutil.virtual_memory()\n",
        "    print(f\"Total RAM: {ram_info.total / (1024 ** 3):.2f} GB\")\n",
        "    print(f\"Available RAM: {ram_info.available / (1024 ** 3):.2f} GB\")\n",
        "\n",
        "display_system_info()\n",
        "\n",
        "# Cell 5: Load Datasets\n",
        "print(\"Loading SQuAD dataset...\")\n",
        "squad_dataset = load_dataset(\"squad\")\n",
        "\n",
        "print(\"Loading custom dataset...\")\n",
        "df = pd.read_excel('/content/datasetQA.xlsx')\n",
        "print(f\"Custom dataset loaded with {len(df)} rows\")\n",
        "\n",
        "# Cell 6: Initialize Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"aubmindlab/bert-base-arabertv2\",\n",
        "    use_fast=True,\n",
        "    model_max_length=512\n",
        ")\n",
        "\n",
        "# Cell 7: Preprocessing Functions\n",
        "def preprocess_squad(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    contexts = [c.strip() for c in examples[\"context\"]]\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        contexts,\n",
        "        max_length=512,\n",
        "        truncation=\"only_second\",\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        sample_idx = sample_map[i]\n",
        "        answer = examples[\"answers\"][sample_idx]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = start_char + len(answer[\"text\"][0])\n",
        "\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        context_start = sequence_ids.index(1) if 1 in sequence_ids else -1\n",
        "        context_end = sequence_ids.index(1, context_start + 1) if 1 in sequence_ids[context_start + 1:] else len(sequence_ids) - 1\n",
        "\n",
        "        if context_start == -1 or offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs\n",
        "\n",
        "def preprocess_custom_dataset(examples):\n",
        "    questions = [str(q) if pd.notnull(q) else '' for q in examples[\"question\"]]\n",
        "    contexts = [str(c) if pd.notnull(c) else '' for c in examples[\"context\"]]\n",
        "    answers = examples['answer']\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        contexts,\n",
        "        max_length=512,\n",
        "        truncation=\"only_second\",\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        sample_idx = sample_map[i]\n",
        "        answer = answers[sample_idx]\n",
        "        if not answer:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "            continue\n",
        "\n",
        "        context = contexts[sample_idx]\n",
        "        start_char = context.find(answer)\n",
        "        end_char = start_char + len(answer)\n",
        "\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        context_start = sequence_ids.index(1) if 1 in sequence_ids else -1\n",
        "        context_end = sequence_ids.index(1, context_start + 1) if 1 in sequence_ids[context_start + 1:] else len(sequence_ids) - 1\n",
        "\n",
        "        if start_char == -1 or context_start == -1 or offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs\n",
        "\n",
        "# Cell 8: Process Datasets\n",
        "print(\"Processing SQuAD dataset...\")\n",
        "processed_squad = squad_dataset.map(\n",
        "    preprocess_squad,\n",
        "    remove_columns=squad_dataset[\"train\"].column_names,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    num_proc=4\n",
        ")\n",
        "\n",
        "print(\"Processing custom dataset...\")\n",
        "custom_dataset = Dataset.from_pandas(df)\n",
        "processed_custom = custom_dataset.map(\n",
        "    preprocess_custom_dataset,\n",
        "    remove_columns=custom_dataset.column_names,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    num_proc=4\n",
        ")\n",
        "\n",
        "# Split custom dataset\n",
        "custom_train, custom_val = processed_custom.train_test_split(test_size=0.1).values()\n",
        "\n",
        "# Cell 9: Initialize Model with LoRA\n",
        "print(\"Initializing model...\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\n",
        "    \"aubmindlab/bert-base-arabertv2\",\n",
        "    return_dict=True\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"QUESTION_ANS\",\n",
        "    target_modules=[\"query\", \"key\", \"value\"],\n",
        "    bias=\"none\",\n",
        "    modules_to_save=[\"qa_outputs\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Cell 10: Training Configuration\n",
        "def get_training_args(output_dir, name):\n",
        "    return TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        run_name=name,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=100,\n",
        "        logging_steps=50,\n",
        "        learning_rate=5e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        gradient_accumulation_steps=2,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        report_to=\"wandb\",\n",
        "        fp16=True,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=200,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        warmup_ratio=0.1,\n",
        "        group_by_length=True,\n",
        "        dataloader_num_workers=2,\n",
        "        gradient_checkpointing=False,\n",
        "        optim=\"adamw_torch\"\n",
        "    )\n",
        "\n",
        "# Cell 11: Training\n",
        "print(\"Training on SQuAD...\")\n",
        "squad_args = get_training_args('/content/squad_model', \"squad_pretraining\")\n",
        "squad_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=squad_args,\n",
        "    train_dataset=processed_squad[\"train\"],\n",
        "    eval_dataset=processed_squad[\"validation\"],\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        ")\n",
        "\n",
        "squad_trainer.train()\n",
        "\n",
        "# Clear cache between training phases\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nFine-tuning on custom dataset...\")\n",
        "custom_args = get_training_args('/content/final_model', \"custom_finetuning\")\n",
        "custom_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=custom_args,\n",
        "    train_dataset=custom_train,\n",
        "    eval_dataset=custom_val,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        ")\n",
        "\n",
        "custom_trainer.train()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zZzx9vm3C_Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC4S3Jxye4M1"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}